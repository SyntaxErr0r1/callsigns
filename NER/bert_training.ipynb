{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7s5qlqI-etnc","trusted":true},"outputs":[],"source":["pip install transformers datasets tokenizers seqeval pyarrow -q"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install --upgrade datasets pyarrow"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip show pyarrow"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# hack\n","!conda install -c conda-forge pyarrow -y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-10T08:09:41.335969Z","iopub.status.busy":"2023-01-10T08:09:41.331052Z","iopub.status.idle":"2023-01-10T08:09:54.635208Z","shell.execute_reply":"2023-01-10T08:09:54.634200Z","shell.execute_reply.started":"2023-01-10T08:09:41.335886Z"},"id":"KooHZMYXeymA","outputId":"aaefe215-3f3c-4f37-e7b7-b423ffc36367","trusted":true},"outputs":[],"source":["import datasets \n","from datasets import Dataset, load_dataset, DatasetDict\n","import numpy as np \n","from transformers import BertTokenizerFast \n","from transformers import DataCollatorForTokenClassification \n","from transformers import AutoModelForTokenClassification \n","from datasets import Dataset\n","\n","conll2003 = datasets.load_dataset(\"conll2003\") \n","atco = load_dataset('json', data_files={\n","    'train': '/kaggle/input/allv3/train.json',\n","    'validation': '/kaggle/input/allv3/validation.json',\n","    'test': '/kaggle/input/allv3/test.json'\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-10T08:10:06.033631Z","iopub.status.busy":"2023-01-10T08:10:06.033250Z","iopub.status.idle":"2023-01-10T08:10:06.041935Z","shell.execute_reply":"2023-01-10T08:10:06.040898Z","shell.execute_reply.started":"2023-01-10T08:10:06.033599Z"},"id":"z9A7Jy37_AJ9","outputId":"c1436dd8-5a23-4441-fc91-41ff365d8260","trusted":true},"outputs":[],"source":["atco"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPdTN25De8qy","outputId":"dbc85372-e4fb-49a7-a9bb-5f9bd93e0711","trusted":true},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\") "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FX3JmnRfAv7","trusted":true},"outputs":[],"source":["def tokenize_and_align_labels(examples, label_all_tokens=True): \n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True) \n","    labels = [] \n","    for i, label in enumerate(examples[\"ner_tags\"]): \n","        word_ids = tokenized_inputs.word_ids(batch_index=i) \n","        previous_word_idx = None \n","        label_ids = []\n","        for word_idx in word_ids: \n","            if word_idx is None: \n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(label[word_idx]) \n","            else: \n","                label_ids.append(label[word_idx] if label_all_tokens else -100) \n","            previous_word_idx = word_idx \n","        labels.append(label_ids) \n","    tokenized_inputs[\"labels\"] = labels \n","    return tokenized_inputs "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kg0c2D7yfbf0","outputId":"0110b5f4-d9c7-4fce-f404-e47449a35bf3","trusted":true},"outputs":[],"source":["tokenized_datasets = atco.map(tokenize_and_align_labels, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSF7QRDQffPB","outputId":"fd402893-085f-462e-c90f-84b308ddc102","trusted":true},"outputs":[],"source":["\n","model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okyF8VWWfhnv","trusted":true},"outputs":[],"source":["from transformers import TrainingArguments, Trainer \n","args = TrainingArguments( \n","\"test-ner\",\n","evaluation_strategy = \"epoch\", \n","learning_rate=2e-5, \n","per_device_train_batch_size=16, \n","per_device_eval_batch_size=16, \n","num_train_epochs=3, \n","weight_decay=0.01, \n",") "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3fNcCdRfkV0","trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sG9bdKiifovx","outputId":"eb1fb21e-6454-4e59-9a66-b601de918890","trusted":true},"outputs":[],"source":["metric = datasets.load_metric(\"seqeval\") "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-tpGx0sft-q","trusted":true},"outputs":[],"source":["\n","example = atco['train'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vom5Yo3LfqoN","outputId":"f539c7a3-c429-4cb1-c07a-fced2c37d1f3","trusted":true},"outputs":[],"source":["\n","label_list = ['O', 'CSG']\n","\n","label_list"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["example[\"ner_tags\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R11EMwO2_AKG","outputId":"943ade34-1be5-45eb-cab4-c334a2adc04b","trusted":true},"outputs":[],"source":["\n","\n","labels = [label_list[i] for i in example[\"ner_tags\"]] \n","\n","metric.compute(predictions=[labels], references=[labels]) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tySgh2eY_AKG","trusted":true},"outputs":[],"source":["def compute_metrics(eval_preds): \n","    pred_logits, labels = eval_preds \n","    \n","    pred_logits = np.argmax(pred_logits, axis=2) \n","    # the logits and the probabilities are in the same order,\n","    # so we donâ€™t need to apply the softmax\n","    \n","    # We remove all the values where the label is -100\n","    predictions = [ \n","        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n","        for prediction, label in zip(pred_logits, labels) \n","    ] \n","    \n","    true_labels = [ \n","      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n","       for prediction, label in zip(pred_logits, labels) \n","   ] \n","    results = metric.compute(predictions=predictions, references=true_labels) \n","    return { \n","   \"precision\": results[\"overall_precision\"], \n","   \"recall\": results[\"overall_recall\"], \n","   \"f1\": results[\"overall_f1\"], \n","  \"accuracy\": results[\"overall_accuracy\"], \n","} "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pNZJvlI3f3dr","trusted":true},"outputs":[],"source":["trainer = Trainer( \n","    model, \n","    args, \n","   train_dataset=tokenized_datasets[\"train\"], \n","   eval_dataset=tokenized_datasets[\"validation\"], \n","   data_collator=data_collator, \n","   tokenizer=tokenizer, \n","   compute_metrics=compute_metrics \n",") "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LbUh9shgD5X","outputId":"d674279e-e988-4837-a0be-a1acb01ba002","trusted":true},"outputs":[],"source":["trainer.train() "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions, label_ids, metrics = trainer.predict(atco[\"test\"])\n","# print(preds_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGmSRYR6L3Ok","outputId":"abde8cf0-1281-4bc3-a5e9-0e5c80302cdb","trusted":true},"outputs":[],"source":["model.save_pretrained(\"ner_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NX9FrmzaR2XW","outputId":"8ba9da08-69dd-4b4b-ba19-dd7da2ca5fcf","trusted":true},"outputs":[],"source":["tokenizer.save_pretrained(\"tokenizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWj16dJeMj10","trusted":true},"outputs":[],"source":["id2label = {\n","    str(i): label for i,label in enumerate(label_list)\n","}\n","label2id = {\n","    label: str(i) for i,label in enumerate(label_list)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8UMh-WdMfwc","trusted":true},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T5EZxnPxMjSb","trusted":true},"outputs":[],"source":["config = json.load(open(\"ner_model/config.json\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_p_ohBAMyQ7","trusted":true},"outputs":[],"source":["config[\"id2label\"] = id2label\n","config[\"label2id\"] = label2id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBagjGQfM3xo","trusted":true},"outputs":[],"source":["json.dump(config, open(\"ner_model/config.json\",\"w\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kLZ-CYWMSNm","outputId":"8e9bb61d-9973-4d45-c278-942dc00007d9","trusted":true},"outputs":[],"source":["model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip show transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2OmIa1_R-Nj","trusted":true},"outputs":[],"source":["from transformers import pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import shutil\n","shutil.make_archive(\"output\", 'zip', \"./\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QH9hlOroHKyN","outputId":"5c6c2739-20d3-472a-a8e5-b87e1a427645","trusted":true},"outputs":[],"source":["nlp = pipeline(\"token-classification\", model=model_fine_tuned, tokenizer=tokenizer)\n","\n","model_fine_tuned\n","\n","\n","example = \"Alpha Charlie Zero Three You be leave tma praha\"\n","\n","ner_results = nlp(example)\n","\n","for res in ner_results:\n","    print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print(\" \".join(atco[\"test\"][0]['tokens']))\n","\n","for res in nlp(\"Alpha Charlie Zero Three You be leave tma praha switch to praha info one two six decimal one naslysenou\"):\n","    final_str = res[\"word\"]\n","    if res[\"score\"] > 0.5:\n","        final_str += res[\"entity\"]\n","    print(final_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 0, 0, 0, 0, 0]])\n","attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])\n","output = model_fine_tuned(input_ids, attention_mask=attention_mask)\n","print(output.logits)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}

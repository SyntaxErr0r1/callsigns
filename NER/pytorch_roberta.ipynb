{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T20:13:41.156557Z","iopub.status.busy":"2022-11-29T20:13:41.155705Z","iopub.status.idle":"2022-11-29T20:13:53.217796Z","shell.execute_reply":"2022-11-29T20:13:53.216279Z","shell.execute_reply.started":"2022-11-29T20:13:41.156498Z"},"trusted":true},"outputs":[],"source":["%%capture\n","!pip install transformers\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T20:13:53.226045Z","iopub.status.busy":"2022-11-29T20:13:53.221581Z","iopub.status.idle":"2022-11-29T20:13:53.236150Z","shell.execute_reply":"2022-11-29T20:13:53.235050Z","shell.execute_reply.started":"2022-11-29T20:13:53.225997Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch \n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from torch.optim import SGD"]},{"cell_type":"markdown","metadata":{},"source":["# Read Spacy Data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T20:13:53.243201Z","iopub.status.busy":"2022-11-29T20:13:53.239895Z","iopub.status.idle":"2022-11-29T20:13:55.961038Z","shell.execute_reply":"2022-11-29T20:13:55.960007Z","shell.execute_reply.started":"2022-11-29T20:13:53.243135Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48bd7c2f60184b58b91b60c56f4693ad","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70d99f35534c4d118bf186ddd4d763b4","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f36a917ed42f4b17b2e4a5298a2c3e07","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29b846a35e564cada21dcf064538f3ed","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import spacy\n","import en_core_web_sm\n","from spacy.tokens import DocBin\n","from spacy import displacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)\n","\n","unique_labels = ['', 'CALLSIGN']\n","\n","labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n","ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n","\n","label_all_tokens = False\n","\n","def load_dataset(data_path):\n","    db = DocBin().from_disk(data_path)\n","    docs = list(db.get_docs(nlp.vocab))\n","    dataset = []\n","\n","    for doc in docs:\n","        #convert doc to list of tokens\n","        tokens = [token.text for token in doc]\n","        #convert doc to list of tags\n","        tags = [token.ent_type_ for token in doc]\n","        # print the dependency tree\n","\n","        utterance = []\n","        utterance.append(tokens)\n","        utterance.append(tags)\n","\n","        dataset.append(utterance)\n","    \n","    return dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Create Dataset Class "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T20:13:55.963820Z","iopub.status.busy":"2022-11-29T20:13:55.963425Z","iopub.status.idle":"2022-11-29T20:13:55.976888Z","shell.execute_reply":"2022-11-29T20:13:55.975132Z","shell.execute_reply.started":"2022-11-29T20:13:55.963781Z"},"trusted":true},"outputs":[],"source":["\n","\n","def align_label(tokens, labels):\n","    # tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n","    tokenized_inputs = tokenizer(tokens, is_split_into_words=True, padding='max_length', max_length=128, truncation=True)\n","\n","    word_ids = tokenized_inputs.word_ids()\n","\n","    previous_word_idx = None\n","    label_ids = []\n","\n","    for word_idx in word_ids:\n","\n","        if word_idx is None:\n","            label_ids.append(-100)\n","\n","        elif word_idx != previous_word_idx:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]])\n","            except:\n","                label_ids.append(-100)\n","        else:\n","            try:\n","                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n","            except:\n","                label_ids.append(-100)\n","        previous_word_idx = word_idx\n","\n","    return label_ids\n","\n","class DataSequence(torch.utils.data.Dataset):\n","\n","    def __init__(self, df):\n","        #for each row in the dataframe, get the text and the label\n","        txt = [d[0] for d in df]\n","        lb = [d[1] for d in df]\n","\n","        self.texts = [tokenizer(str(i),\n","                               padding='max_length', max_length = 128, truncation=True, return_tensors=\"pt\") for i in txt]\n","        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n","\n","    def __len__(self):\n","\n","        return len(self.labels)\n","\n","    def get_batch_data(self, idx):\n","\n","        return self.texts[idx]\n","\n","    def get_batch_labels(self, idx):\n","\n","        return torch.LongTensor(self.labels[idx])\n","\n","    def __getitem__(self, idx):\n","\n","        batch_data = self.get_batch_data(idx)\n","        batch_labels = self.get_batch_labels(idx)\n","\n","        return batch_data, batch_labels"]},{"cell_type":"markdown","metadata":{},"source":["# Split Data and Define Unique Labels"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T21:38:02.501740Z","iopub.status.busy":"2022-11-29T21:38:02.501352Z","iopub.status.idle":"2022-11-29T21:38:04.063124Z","shell.execute_reply":"2022-11-29T21:38:04.062091Z","shell.execute_reply.started":"2022-11-29T21:38:02.501706Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["({'input_ids': tensor([[    0, 47052,   366,  5901,  3934,   128,   330, 11313,  3934,   128,\n","         21466, 11483, 12179,  3934,   128,   611, 11278,   324,  3695,  3934,\n","           128, 29135,  3934,   128,  6968,  3934,   128,  1322,  3934,   128,\n","           459, 13286,  3934,   128,    90,  1916,  3934,   128,   642,   763,\n","          1999,  3934,   128, 43067,  3934,   128,   560,  3934,   128,   642,\n","           763,  1999,  3934,   128, 23999,  3934,   128,  1264,  3934,   128,\n","          7109,  3934,   128, 13664,  3934,   128, 11127, 16980,  3934,   128,\n","          1264,  3934,   128,  3695,  3934,   128, 22118,   352,  7305,  1438,\n","         44403,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","             1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])}, tensor([-100,    1, -100,    1, -100,    1, -100, -100,    1, -100, -100,    1,\n","           0,    0,    0,    0, -100,    0, -100,    0,    0,    0, -100,    0,\n","           0,    0,    0,    0,    0,    0, -100,    0, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n","        -100, -100, -100, -100, -100, -100, -100, -100]))\n"]}],"source":["df_train = load_dataset(\"/kaggle/input/all-751515-lowercase/train.spacy\")\n","df_val = load_dataset(\"/kaggle/input/all-751515-lowercase/validation.spacy\")\n","df_test = load_dataset(\"/kaggle/input/all-751515-lowercase/test.spacy\")\n","\n","train_dataset = DataSequence(df_train)\n","\n","#print the first row of the dataset and the original text\n","print(train_dataset[0])\n","\n","\n","\n","# df = df[0:1000]\n","\n","# labels = [i.split() for i in df['labels'].values.tolist()]\n","# unique_labels = set()\n","\n","# for lb in labels:\n","#         [unique_labels.add(i) for i in lb if i not in unique_labels]\n","# labels_to_ids = {k: v for v, k in enumerate(unique_labels)}\n","# ids_to_labels = {v: k for v, k in enumerate(unique_labels)}\n","\n","# df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n","#                             [int(.8 * len(df)), int(.9 * len(df))])"]},{"cell_type":"markdown","metadata":{},"source":["# Build Model"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T21:38:08.957135Z","iopub.status.busy":"2022-11-29T21:38:08.956363Z","iopub.status.idle":"2022-11-29T21:38:08.964703Z","shell.execute_reply":"2022-11-29T21:38:08.963373Z","shell.execute_reply.started":"2022-11-29T21:38:08.957086Z"},"trusted":true},"outputs":[],"source":["class BertModel(torch.nn.Module):\n","\n","    def __init__(self):\n","\n","        super(BertModel, self).__init__()\n","        self.bert = AutoModelForMaskedLM.from_pretrained(\"roberta-base\")\n","        # self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n","\n","    def forward(self, input_id, mask, label):\n","\n","        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T18:36:42.446765Z","iopub.status.busy":"2022-11-29T18:36:42.446336Z","iopub.status.idle":"2022-11-29T18:36:43.506396Z","shell.execute_reply":"2022-11-29T18:36:43.505214Z","shell.execute_reply.started":"2022-11-29T18:36:42.446722Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":[]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T21:38:15.307352Z","iopub.status.busy":"2022-11-29T21:38:15.306986Z","iopub.status.idle":"2022-11-29T21:53:47.030649Z","shell.execute_reply":"2022-11-29T21:53:47.028633Z","shell.execute_reply.started":"2022-11-29T21:38:15.307322Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 1 | Loss:  0.868 | Accuracy:  0.553 | Val_Loss:  0.657 | Accuracy:  0.608\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 2 | Loss:  0.659 | Accuracy:  0.606 | Val_Loss:  0.619 | Accuracy:  0.651\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.35it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 3 | Loss:  0.645 | Accuracy:  0.620 | Val_Loss:  0.588 | Accuracy:  0.657\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 4 | Loss:  0.610 | Accuracy:  0.629 | Val_Loss:  0.635 | Accuracy:  0.608\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 5 | Loss:  0.524 | Accuracy:  0.716 | Val_Loss:  0.667 | Accuracy:  0.608\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 6 | Loss:  0.325 | Accuracy:  0.859 | Val_Loss:  0.229 | Accuracy:  0.903\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 7 | Loss:  0.237 | Accuracy:  0.903 | Val_Loss:  0.182 | Accuracy:  0.927\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 8 | Loss:  0.180 | Accuracy:  0.930 | Val_Loss:  0.196 | Accuracy:  0.925\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 9 | Loss:  0.157 | Accuracy:  0.939 | Val_Loss:  0.151 | Accuracy:  0.942\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 10 | Loss:  0.137 | Accuracy:  0.946 | Val_Loss:  0.154 | Accuracy:  0.945\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 11 | Loss:  0.129 | Accuracy:  0.952 | Val_Loss:  0.155 | Accuracy:  0.951\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 12 | Loss:  0.112 | Accuracy:  0.958 | Val_Loss:  0.142 | Accuracy:  0.946\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 13 | Loss:  0.100 | Accuracy:  0.963 | Val_Loss:  0.142 | Accuracy:  0.952\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 14 | Loss:  0.089 | Accuracy:  0.967 | Val_Loss:  0.172 | Accuracy:  0.951\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 15 | Loss:  0.087 | Accuracy:  0.968 | Val_Loss:  0.143 | Accuracy:  0.952\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 985/985 [00:53<00:00, 18.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 16 | Loss:  0.077 | Accuracy:  0.973 | Val_Loss:  0.154 | Accuracy:  0.954\n"]}],"source":["\n","def train_loop(model, df_train, df_val):\n","\n","    train_dataset = DataSequence(df_train)\n","    val_dataset = DataSequence(df_val)\n","\n","    train_dataloader = DataLoader(train_dataset, num_workers=2, batch_size=BATCH_SIZE, shuffle=True)\n","    val_dataloader = DataLoader(val_dataset, num_workers=2, batch_size=BATCH_SIZE)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    best_acc = 0\n","    best_loss = 1000\n","\n","    for epoch_num in range(EPOCHS):\n","\n","        total_acc_train = 0\n","        total_loss_train = 0\n","\n","        model.train()\n","\n","        for train_data, train_label in tqdm(train_dataloader):\n","\n","            train_label = train_label.to(device)\n","            mask = train_data['attention_mask'].squeeze(1).to(device)\n","            input_id = train_data['input_ids'].squeeze(1).to(device)\n","\n","            optimizer.zero_grad()\n","            loss, logits = model(input_id, mask, train_label)\n","\n","            for i in range(logits.shape[0]):\n","\n","              logits_clean = logits[i][train_label[i] != -100]\n","              label_clean = train_label[i][train_label[i] != -100]\n","\n","              predictions = logits_clean.argmax(dim=1)\n","              acc = (predictions == label_clean).float().mean()\n","              total_acc_train += acc\n","              total_loss_train += loss.item()\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","\n","        total_acc_val = 0\n","        total_loss_val = 0\n","\n","        for val_data, val_label in val_dataloader:\n","\n","            val_label = val_label.to(device)\n","            mask = val_data['attention_mask'].squeeze(1).to(device)\n","            input_id = val_data['input_ids'].squeeze(1).to(device)\n","\n","            loss, logits = model(input_id, mask, val_label)\n","\n","            for i in range(logits.shape[0]):\n","\n","              logits_clean = logits[i][val_label[i] != -100]\n","              label_clean = val_label[i][val_label[i] != -100]\n","\n","              predictions = logits_clean.argmax(dim=1)\n","              acc = (predictions == label_clean).float().mean()\n","              total_acc_val += acc\n","              total_loss_val += loss.item()\n","\n","        val_accuracy = total_acc_val / len(df_val)\n","        val_loss = total_loss_val / len(df_val)\n","\n","        print(\n","            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n","\n","LEARNING_RATE = 5e-3\n","EPOCHS = 16\n","BATCH_SIZE = 2\n","\n","model = BertModel()\n","train_loop(model, df_train, df_val)"]},{"cell_type":"markdown","metadata":{},"source":["## Save/Load Model"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T20:29:59.486524Z","iopub.status.busy":"2022-11-29T20:29:59.486131Z","iopub.status.idle":"2022-11-29T20:30:00.760662Z","shell.execute_reply":"2022-11-29T20:30:00.759109Z","shell.execute_reply.started":"2022-11-29T20:29:59.486488Z"},"trusted":true},"outputs":[],"source":["model.eval()\n","torch.save(model, \"model.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model2 = torch.load(\"model.pt\", map_location=torch.device('cuda'))\n","model2.eval()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate Model"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T22:00:09.227525Z","iopub.status.busy":"2022-11-29T22:00:09.227143Z","iopub.status.idle":"2022-11-29T22:00:16.664226Z","shell.execute_reply":"2022-11-29T22:00:16.662985Z","shell.execute_reply.started":"2022-11-29T22:00:09.227482Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------\n","Predictions:  tensor([1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n","Labels:  tensor([1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n","Accuracy:  tensor(1., device='cuda:0')\n","-----------------\n","Predictions:  tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n","Labels:  tensor([0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n","Accuracy:  tensor(1., device='cuda:0')\n","-----------------\n","Predictions:  tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","Labels:  tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","Accuracy:  tensor(1., device='cuda:0')\n","-----------------\n","Predictions:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n","Labels:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n","Accuracy:  tensor(1., device='cuda:0')\n","-----------------\n","Predictions:  tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","Labels:  tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n","Accuracy:  tensor(0.9231, device='cuda:0')\n","Test Accuracy:  0.952\n"]}],"source":["def evaluate(model, df_test):\n","\n","    test_dataset = DataSequence(df_test)\n","\n","    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","\n","    total_acc_test = 0.0\n","\n","    iterator = 0\n","\n","    for test_data, test_label in test_dataloader:\n","            \n","\n","        test_label = test_label.to(device)\n","        mask = test_data['attention_mask'].squeeze(1).to(device)\n","\n","        input_id = test_data['input_ids'].squeeze(1).to(device)\n","\n","        loss, logits = model(input_id, mask, test_label)\n","\n","        for i in range(logits.shape[0]):\n","\n","            logits_clean = logits[i][test_label[i] != -100]\n","            label_clean = test_label[i][test_label[i] != -100]\n","\n","            predictions = logits_clean.argmax(dim=1)\n","            acc = (predictions == label_clean).float().mean()\n","            total_acc_test += acc\n","            if iterator < 10:\n","              print(\"-----------------\")\n","              print('Predictions: ', predictions)\n","              print('Labels: ', label_clean)\n","              print('Accuracy: ', acc)\n","              iterator += 1\n","\n","        iterator += 1\n","\n","    val_accuracy = total_acc_test / len(df_test)\n","    print(f'Test Accuracy: {total_acc_test / len(df_test): .3f}')\n","\n","\n","evaluate(model, df_test)"]},{"cell_type":"markdown","metadata":{},"source":["# Predict One Sentence"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2022-11-29T22:02:37.992687Z","iopub.status.busy":"2022-11-29T22:02:37.992267Z","iopub.status.idle":"2022-11-29T22:02:38.488857Z","shell.execute_reply":"2022-11-29T22:02:38.487740Z","shell.execute_reply.started":"2022-11-29T22:02:37.992650Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['Foxtrot Charlie Kilo Two Charlie Bravo Descent Level Two Hundred', tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')]\n"]}],"source":["def evaluate_return(model, df_test):\n","\n","    test_dataset = DataSequence(df_test)\n","\n","    test_dataloader = DataLoader(test_dataset, num_workers=4, batch_size=1)\n","\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","    if use_cuda:\n","        model = model.cuda()\n","    \n","    results = []\n","\n","    for test_data, test_label in test_dataloader:\n","            \n","\n","        test_label = test_label.to(device)\n","        mask = test_data['attention_mask'].squeeze(1).to(device)\n","\n","        input_id = test_data['input_ids'].squeeze(1).to(device)\n","\n","        loss, logits = model(input_id, mask, test_label)\n","\n","        for i in range(logits.shape[0]):\n","            \n","            result = []\n","\n","            logits_clean = logits[i][test_label[i] != -100]\n","            label_clean = test_label[i][test_label[i] != -100]\n","\n","            predictions = logits_clean.argmax(dim=1)\n","            \n","            original_sentence = \"\"\n","            for d in df_test:\n","                original_sentence = ' '.join(d[0])\n","            \n","            result.append(original_sentence)\n","            result.append(predictions)\n","            \n","            results.append(result)\n","    return results\n","\n","def evaluate_single(model,text):\n","    tokens = text.split()\n","    tags=[''] * len(tokens)\n","\n","    test_data = []\n","    test_data.append(tokens)\n","    test_data.append(tags)\n","\n","    test_df = []\n","    test_df.append(test_data)\n","\n","    return evaluate_return(model, test_df)[0]\n","\n","print(evaluate_single(model,\"Foxtrot Charlie Kilo Two Charlie Bravo Descent Level Two Hundred\"))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"}}},"nbformat":4,"nbformat_minor":5}
